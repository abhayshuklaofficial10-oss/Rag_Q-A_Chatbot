ğŸ‘‡

ğŸ§  Custom Conversational RAG Q&A Chatbot

A conversational Retrieval-Augmented Generation (RAG) chatbot built with LangChain LCEL, Groq LLaMA 3, and Streamlit, designed to answer questions accurately from uploaded PDF documents while maintaining chat history and context awareness.

Unlike basic RAG systems, this chatbot supports multi-turn conversations by intelligently rephrasing follow-up questions into standalone queries before retrieval.

ğŸš€ Key Features

ğŸ“„ Multi-PDF Upload & Processing â€“ Upload and chat with multiple PDF documents

ğŸ§  Conversational Memory Support â€“ Maintains chat history using LangChain message objects

ğŸ”„ Question Condensation â€“ Converts follow-up questions into standalone queries for better retrieval

ğŸ” Semantic Search with Embeddings â€“ Uses HuggingFace embeddings for accurate document retrieval

ğŸ—‚ï¸ Vector Store Integration â€“ Stores and retrieves document chunks using Chroma DB

âš¡ Fast LLM Inference â€“ Powered by Groqâ€™s LLaMA 3.1 for low-latency responses

ğŸ¯ Hallucination Control â€“ Answers strictly from retrieved context or clearly states when the answer is unknown

ğŸ–¥ï¸ Interactive UI â€“ Clean chat-based interface built with Streamlit

ğŸ› ï¸ Tech Stack

ğŸ§  LLM: Groq â€“ LLaMA 3.1 (8B Instant)

ğŸ”— Framework: LangChain (LCEL-based pipelines)

ğŸ“ Embeddings: HuggingFace (all-MiniLM-L6-v2)

ğŸ—ƒï¸ Vector Database: Chroma

ğŸ“„ Document Loader: PyPDFLoader

âœ‚ï¸ Text Splitting: RecursiveCharacterTextSplitter

ğŸ§ª Prompting: ChatPromptTemplate + Message Placeholders

ğŸ–¥ï¸ Frontend: Streamlit

ğŸ Language: Python
